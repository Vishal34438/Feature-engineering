{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015933b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Engineering Example - Titanic Dataset\n",
    "Name: Vishal Shende\n",
    "PRN:202401110034\n",
    "AIML(A2)\n",
    "# ---------------------------------------------\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")\n",
    "\n",
    "\n",
    "print(\"✅ Dataset Loaded Successfully!\\n\")\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 1: Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 2: Handle missing values using imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data['Age'] = imputer.fit_transform(data[['Age']])\n",
    "data['Fare'] = imputer.fit_transform(data[['Fare']])\n",
    "data['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 3: Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data['Sex'] = label_encoder.fit_transform(data['Sex'])  # Male=1, Female=0\n",
    "data = pd.get_dummies(data, columns=['Embarked'], drop_first=True)  # One-hot encoding\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 4: Feature scaling\n",
    "scaler = StandardScaler()\n",
    "data[['Age', 'Fare']] = scaler.fit_transform(data[['Age', 'Fare']])\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 5: Dimensionality reduction using PCA (optional)\n",
    "pca = PCA(n_components=2)\n",
    "pca_features = pca.fit_transform(data[['Age', 'Fare']])\n",
    "data['PCA1'] = pca_features[:, 0]\n",
    "data['PCA2'] = pca_features[:, 1]\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 6: Feature selection (SelectKBest)\n",
    "X = data[['Pclass', 'Sex', 'Age', 'Fare', 'SibSp', 'Parch']]\n",
    "y = data['Survived']\n",
    "\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=3)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"\\nSelected Important Features:\", selected_features.tolist())\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Step 7: Summary\n",
    "print(\"\\n✅ Feature Engineering Completed Successfully!\")\n",
    "print(\"Transformed dataset shape:\", data.shape)\n",
    "print(\"Few rows after transformation:\")\n",
    "print(data.head())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
